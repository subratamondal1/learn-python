{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce56777-7217-4ec5-a075-edd53fcfb53a",
   "metadata": {},
   "source": [
    "# Linear Neural Networks for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dae6a3-4d10-4e29-ab92-b195f664dc19",
   "metadata": {},
   "source": [
    "Before we worry about making our neural networks **deep**, it will be helpful to implement some **shallow** ones, for which the inputs connect directly to the outputs. This will prove important for a few reasons. \n",
    "- First, rather than getting distracted by complicated architectures, we can focus on the basics of neural network training, including **parametrizing** the output layer, handling data, specifying a loss function, and training the model.\n",
    "- Second, this class of shallow networks happens to comprise the set of linear models, which subsumes many classical methods of statistical prediction, including **linear** and **softmax regression**.\n",
    "\n",
    "Understanding these classical tools is pivotal because they are widely used in many contexts and we will often need to use them as baselines when justifying the use of fancier architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1af01-c802-49cd-b609-84e40ff86fb1",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26cff3-c325-435c-a20a-8ec9acf37c21",
   "metadata": {},
   "source": [
    "Regression problems pop up whenever we want to predict a numerical value. Common examples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for patients in the hospital), forecasting demand (for retail sales), among numerous others. Not every prediction problem is one of classical regression. Later on, we will introduce classification problems, where the goal is to predict membership among a set of categories.\n",
    "\n",
    "As a running example, suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To develop a model for predicting house prices, we need to get our hands on data, including the sales price, area, and age for each home. In the terminology of machine learning, the dataset is called a **training dataset** or **training set**, and each row (containing the data corresponding to one sale) is called an **example** (or data point, instance, sample). The thing we are trying to predict (price) is called a **label** (or **target**). The variables (age and area) upon which the predictions are based are called **features** (or **covariates**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3bef7a-ca64-4f8a-8fe1-c3c376fbbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878cdd2-12ec-496d-934d-ca48235d7977",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154173d-d98b-4a9d-98dc-c22637041875",
   "metadata": {},
   "source": [
    "<img src=\"assets/lr1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f455d-c1fd-4b87-9819-0f07a8a4df4b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fc6a4-5c06-417b-b098-029d6454f718",
   "metadata": {},
   "source": [
    "<img src=\"assets/model1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb65e8-435c-46cb-95a7-f2f4a86adb15",
   "metadata": {},
   "source": [
    "<img src=\"assets/model2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d78454-1e54-485e-8a00-38f18296c22a",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ab152-e8d4-4d0d-b706-99b9bf3e6f9b",
   "metadata": {},
   "source": [
    "<img src=\"assets/loss fn.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b63d79-c27c-488b-a293-205e2992b818",
   "metadata": {},
   "source": [
    "### Loss Function: $L(\\mathbf{w}, b)$\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "**Explanation**:\n",
    "- **$L(\\mathbf{w}, b)$**: This is the overall loss function, a function of the weights $ \\mathbf{w} $ and bias $ b $.\n",
    "- **$n$**: The number of training examples in the dataset.\n",
    "- **$l^{(i)}(\\mathbf{w}, b)$**: The loss for a single training example $ i $.\n",
    "- **$\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$**: This represents the predicted value $\\hat{y}^{(i)}$ for the $i$-th training example, where $\\mathbf{w}^\\top$ is the transpose of the weights vector multiplied by the feature vector $\\mathbf{x}^{(i)}$, and $b$ is the bias.\n",
    "- **$y^{(i)}$**: The actual target value for the $i$-th training example.\n",
    "- **$\\frac{1}{2} \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)} \\right)^2$**: This is the squared error for the $i$-th training example, divided by 2 for mathematical convenience (it simplifies the derivative calculations).\n",
    "\n",
    "### Optimization Goal: $ \\mathbf{w}^*, b^* = \\arg \\min_{\\mathbf{w}, b} L(\\mathbf{w}, b) $\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^*, b^* = \\arg \\min_{\\mathbf{w}, b} L(\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "**Explanation**:\n",
    "- **$ \\mathbf{w}^*, b^* $**: These are the optimal values for the weights and bias that minimize the loss function.\n",
    "- **$ \\arg \\min_{\\mathbf{w}, b} $**: This notation means \"the values of $ \\mathbf{w} $ and $ b $ that minimize\" the loss function $ L(\\mathbf{w}, b) $.\n",
    "- **Minimization**: The goal of training is to adjust the weights and bias to minimize the loss function, which in turn means the modelâ€™s predictions are as close as possible to the actual target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9c66d-0e54-40aa-8b3e-42d9f5ba7440",
   "metadata": {},
   "source": [
    "<img src=\"assets/loss fn 2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a37fe-2044-486a-aa6c-6cfa96c2d682",
   "metadata": {},
   "source": [
    "### Derivative of the Loss Function\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} \\left\\|\\mathbf{y} - \\mathbf{Xw}\\right\\|^2 = 2\\mathbf{X}^\\top (\\mathbf{Xw} - \\mathbf{y}) = 0\n",
    "$$\n",
    "\n",
    "**Explanation**:\n",
    "- **Purpose**: This function represents the derivative of the loss function with respect to the weight vector $ \\mathbf{w} $. In the context of linear regression, this derivative is set to zero to find the minimum point of the loss function, which gives the optimal weights.\n",
    "- **Derivative of Squared Loss**: The term $ \\left\\|\\mathbf{y} - \\mathbf{Xw}\\right\\|^2 $ is the squared loss, which measures the difference between the actual values $ \\mathbf{y} $ and the predicted values $ \\mathbf{Xw} $. Taking the derivative with respect to $ \\mathbf{w} $ and setting it to zero is how we find the minimum loss.\n",
    "- **Result**: This derivative simplifies to $ \\mathbf{X}^\\top \\mathbf{y} = \\mathbf{X}^\\top \\mathbf{Xw} $. This equation is fundamental in solving for the optimal weights.\n",
    "\n",
    "**Pronunciation**:\n",
    "- \"The derivative of the loss function with respect to the weight vector $ \\mathbf{w} $ equals two times X-transpose multiplied by the difference between Xw and y, set to zero.\"\n",
    "\n",
    "### Analytic Solution for $ \\mathbf{w}^* $\n",
    "$$\n",
    "\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "**Explanation**:\n",
    "- **Purpose**: This formula provides the optimal weights $ \\mathbf{w}^* $ that minimize the loss function in a linear regression problem. It is derived by solving the equation from the first function.\n",
    "- **Matrix Inversion**: $ (\\mathbf{X}^\\top \\mathbf{X})^{-1} $ is the inverse of the matrix $ \\mathbf{X}^\\top \\mathbf{X} $. This inversion is only possible if $ \\mathbf{X}^\\top \\mathbf{X} $ is invertible, meaning the columns of $ \\mathbf{X} $ must be linearly independent.\n",
    "- **Optimal Weights**: The result $ \\mathbf{w}^* $ gives the best possible weights for the model, ensuring that the predictions are as close as possible to the actual values.\n",
    "\n",
    "**Pronunciation**:\n",
    "- \"The optimal weights $ \\mathbf{w}^* $ are equal to the inverse of X-transpose times X, multiplied by X-transpose times y.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905fa13-d2f0-4091-8346-3733f683e4a1",
   "metadata": {},
   "source": [
    "### Minibatch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff79ed-6360-4c35-bb25-41d2a3277421",
   "metadata": {},
   "source": [
    "Training deep learning models can be effective even when analytical solutions are difficult to obtain because these challenging models often **deliver superior results**, making the effort to train them worthwhile. The core technique used for optimization in deep learning is **`gradient descent`**, where **model parameters** are **updated iteratively** to reduce the loss function.\n",
    "\n",
    "Gradient descent can be **slow** when applied to the entire dataset at once, so alternatives like **stochastic gradient descent (SGD)**, which updates based on a **single sample**, are used. However, SGD can be inefficient due to computational and statistical limitations. To balance efficiency and effectiveness, **minibatch SGD** is employed, where updates are made using small batches of samples, typically between **32** and **256** observations.\n",
    "\n",
    "Minibatch SGD involves randomly sampling a minibatch, computing the gradient of the loss function with respect to the model parameters, and updating the parameters in the **direction that reduces the loss**. This process is repeated iteratively, with the minibatch size and learning rate being key hyperparameters that can be tuned.\n",
    "\n",
    "Although the algorithm doesn't always find the exact minimizers of the loss function, it generally leads to parameters that perform well on training data. The true challenge lies in achieving good generalization, where the model makes accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81277834-bf6b-4328-9b89-100d5c9519e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (1772890781.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    The image you've provided illustrates the Minibatch Stochastic Gradient Descent (SGD) algorithm. Letâ€™s break down the formula and explain how it works step by step.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "### Explanation of the Formula in Minibatch Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The image you've provided illustrates the Minibatch Stochastic Gradient Descent (SGD) algorithm. Letâ€™s break down the formula and explain how it works step by step.\n",
    "\n",
    "#### The Formula:\n",
    "$$\n",
    "(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\frac{\\eta}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\nabla_{(\\mathbf{w}, b)} \\ell^{(i)} (\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "#### How to Pronounce/Read the Formula in Natural Language:\n",
    "- The pair of parameters $$(\\mathbf{w}, b)$$ is updated to a new value.\n",
    "- This update is performed by subtracting a quantity from the current values of $$(\\mathbf{w}, b)$$.\n",
    "- The quantity to subtract is the product of the learning rate $$\\eta$$ divided by the size of the minibatch $$|\\mathcal{B}_t|$$, and the average of the gradients of the loss function $$\\ell^{(i)}$$ with respect to the parameters $$(\\mathbf{w}, b)$$ for all samples in the minibatch $$\\mathcal{B}_t$$.\n",
    "\n",
    "#### How the Formula is Working:\n",
    "1. **Initialization**: \n",
    "   - Before the iteration starts, the model parameters $$\\mathbf{w}$$ (weights) and $$b$$ (bias) are initialized. They can be initialized randomly or using other techniques.\n",
    "   \n",
    "2. **Minibatch Sampling**:\n",
    "   - At each iteration $$t$$, a minibatch $$\\mathcal{B}_t$$ of training examples is randomly sampled from the training data. The minibatch $$\\mathcal{B}_t$$ contains a fixed number of examples, denoted by $$|\\mathcal{B}_t|$$.\n",
    "   \n",
    "3. **Compute Gradients**:\n",
    "   - For each example $$i$$ in the minibatch $$\\mathcal{B}_t$$, compute the gradient of the loss function $$\\ell^{(i)}$$ with respect to the model parameters $$\\mathbf{w}$$ and $$b$$. The gradient $$\\nabla_{(\\mathbf{w},b)} \\ell^{(i)}(\\mathbf{w}, b)$$ tells us the direction in which the loss increases. \n",
    "\n",
    "4. **Average the Gradients**:\n",
    "   - The gradients for all examples in the minibatch are averaged. This averaging is done by summing up the individual gradients and dividing by the number of examples in the minibatch $$|\\mathcal{B}_t|$$.\n",
    "   \n",
    "5. **Update the Parameters**:\n",
    "   - The parameters $$\\mathbf{w}$$ and $$b$$ are updated by moving them in the direction opposite to the averaged gradient. The amount by which we move is controlled by the learning rate $$\\eta$$. The learning rate is a small positive number that determines the step size for each update.\n",
    "   \n",
    "6. **Repeat**:\n",
    "   - This process is repeated for many iterations, sampling a new minibatch $$\\mathcal{B}_t$$ each time, until the algorithm converges (i.e., the loss function stops decreasing significantly).\n",
    "\n",
    "#### What Each Part is Doing:\n",
    "- **$$\\mathbf{w}$$ and $$b$$**: These are the parameters (weights and biases) of your model that you're trying to optimize.\n",
    "- **$$\\eta$$ (eta)**: This is the learning rate, a small positive scalar that controls the size of the step we take in the direction of the negative gradient.\n",
    "- **$$|\\mathcal{B}_t|$$**: This denotes the size of the minibatch $$\\mathcal{B}_t$$, i.e., the number of training examples in the minibatch.\n",
    "- **$$\\nabla_{(\\mathbf{w},b)} \\ell^{(i)}(\\mathbf{w}, b)$$**: This is the gradient of the loss function $$\\ell^{(i)}$$ with respect to the parameters $$(\\mathbf{w}, b)$$ for a single training example $$i$$. It tells us how much the loss will change if we slightly change the parameters $$(\\mathbf{w}, b)$$.\n",
    "\n",
    "#### Closed-Form Expansion:\n",
    "The closed-form expansion provided is specific to quadratic loss functions and affine transformations. It shows how the weight vector $$\\mathbf{w}$$ and bias $$b$$ are updated using the input features $$\\mathbf{x}^{(i)}$$ and the actual output $$y^{(i)}$$.\n",
    "\n",
    "This expansion explains the specific update rules for weights and bias:\n",
    "$$$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\n",
    "$$$$\n",
    "$$$$\n",
    "b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\n",
    "$$$$\n",
    "- **$$\\mathbf{x}^{(i)}$$**: The feature vector of the i-th training example.\n",
    "- **$$y^{(i)}$$**: The actual target value for the i-th training example.\n",
    "- **$$\\mathbf{w}^\\top \\mathbf{x}^{(i)}$$**: The predicted output for the i-th training example based on the current model parameters.\n",
    "\n",
    "In summary, minibatch SGD optimizes the model parameters by iteratively updating them in the direction that reduces the average loss over a small, randomly sampled subset (minibatch) of the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ee975-0f7e-46a3-920f-39c24f0b6a1a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
