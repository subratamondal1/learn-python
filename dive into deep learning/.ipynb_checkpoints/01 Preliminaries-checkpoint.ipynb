{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fabac6e-a6ff-4c44-b0fa-4c7ae70e4151",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9975ea-becb-4f38-9936-129983d62e71",
   "metadata": {},
   "source": [
    "To prepare for your dive into deep learning, you will need a few survival skills: \n",
    "- (i) techniques for **storing** and **manipulating** data;\n",
    "- (ii) libraries for **ingesting** and **preprocessing** data from a variety of sources;\n",
    "- (iii) knowledge of the basic linear algebraic operations that we apply to high-dimensional data elements;\n",
    "- (iv) just enough calculus to determine which direction to adjust each parameter in order to decrease the loss function;\n",
    "- (v) the ability to automatically compute derivatives so that you can forget much of the calculus you just learned;\n",
    "- (vi) some basic fluency in probability, our primary language for reasoning under uncertainty;\n",
    "- (vii) some aptitude for finding answers in the official documentation when you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761897ee-99a4-4f24-90d5-f62647d850b9",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bdcaf-e132-4c0d-9c8f-86b88538cb25",
   "metadata": {},
   "source": [
    "In order to get anything done, we need some **way to store** and **manipulate data**. Generally, there are two important things we need to do with data: \n",
    "- (i) acquire them.\n",
    "- (ii) process them once they are inside the computer.\n",
    "\n",
    "There is no point in acquiring data without some way to store it, so to start, let’s get our hands dirty with **n-dimensional arrays**, which we also call **tensors**. If you already know the NumPy scientific computing package, this will be a breeze. For all modern deep learning frameworks, the tensor class (ndarray in MXNet, Tensor in PyTorch and TensorFlow) resembles NumPy’s ndarray, with a few killer features added. \n",
    "- First, the **tensor class supports automatic differentiation**.\n",
    "- Second, it **leverages GPUs to accelerate numerical computation**, whereas NumPy only runs on CPUs.\n",
    "\n",
    "These properties make neural networks both easy to code and fast to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854dac2-e57c-4ca6-847b-d34309eebee4",
   "metadata": {},
   "source": [
    "To start, we import the **PyTorch** library. Note that the package name is **torch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07d8d0f-60ba-43b6-b993-b1aff832db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050f76d-2e6c-45bd-bfd0-01ae454c765c",
   "metadata": {},
   "source": [
    "A tensor represents a (possibly multidimensional) array of numerical values. In the one-dimensional case, i.e., when only one axis is needed for the data, a tensor is called a **vector**. With two axes, a tensor is called a **matrix**. With `k>2` axes, we drop the specialized names and just refer to the object as a $k^{th}$**-order tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c759a-106f-4299-a0ca-41e058dadd7a",
   "metadata": {},
   "source": [
    "### arange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e866c-7d74-4844-8248-b713ebcdecae",
   "metadata": {},
   "source": [
    "PyTorch provides a variety of functions for creating new tensors prepopulated with values. For example, by invoking **arange(n)**, we can create a vector of evenly spaced values, starting at 0 (included) and ending at n (not included). By default, the interval size is 1. Unless otherwise specified, new tensors are stored in main memory and designated for CPU-based computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec5eafc-2df9-4899-82bd-53e1e52efe9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff0c43-98c4-4b39-b1d3-f34cfff29ed0",
   "metadata": {},
   "source": [
    "### numel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a2dd3-dde4-401c-bae9-5d169790a320",
   "metadata": {},
   "source": [
    "Each of these values is called an **element** of the tensor. The tensor x contains 12 elements. We can inspect the total number of elements in a tensor via its **`numel`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad86fee-e4d5-46cc-9c1b-febf60d2da03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df770bc-a00d-439d-ad6f-50020c553942",
   "metadata": {},
   "source": [
    "### shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315d86e-39dc-440b-963c-fb0bc440a79b",
   "metadata": {},
   "source": [
    "We can access a tensor’s shape (the length along each axis) by inspecting its **shape** attribute. Because we are dealing with a vector here, the shape contains just a single element and is identical to the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98e4e7d-98fc-4c5a-b0eb-71e01b3a9858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8415bd-ab0a-4918-a459-c689ab264ddc",
   "metadata": {},
   "source": [
    "### reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb37d14-69ff-4144-bb6d-f6748798f51c",
   "metadata": {},
   "source": [
    "We can change the **shape** of a tensor **without altering its size or values**, by invoking reshape. For example, we can transform our vector x whose shape is (12,) to a matrix X with shape (3, 4). This new tensor retains all elements but reconfigures them into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6c87d9-2228-414a-888e-4539f894fec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ead00-5166-4783-8997-07381c0933ea",
   "metadata": {},
   "source": [
    "Note that specifying every shape component to reshape is redundant. Because we already know our tensor’s size, we can work out one component of the shape given the rest. For example, given a tensor of size `n` and target shape `(h,w)`, we know that `w=n/h`. To automatically infer one component of the **shape**, we can place a **-1** for the shape component that should be **inferred automatically**. In our case, instead of calling x.reshape(3, 4), we could have equivalently called **x.reshape(-1, 4)** or **x.reshape(3, -1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9e2eb7-9ae2-48b5-aebe-cf3a35db21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "X = x.reshape(3, -1)\n",
    "print(X.shape)\n",
    "\n",
    "X = x.reshape(-1, 4)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318863f-e970-472d-be4a-d795cedf07e6",
   "metadata": {},
   "source": [
    "### zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92892642-f59e-4193-83a2-16f5440fcdf1",
   "metadata": {},
   "source": [
    "Practitioners often need to work with tensors initialized to contain all 0s or 1s. We can construct a tensor with all elements set to 0 and a shape of (2, 3, 4) via the **zeros** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050906a0-acfb-4b8a-9d48-32009bfa3625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9d64b-9ea9-4d37-b3a8-06fa8c0ab82f",
   "metadata": {},
   "source": [
    "### ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee112c-1559-4b10-a438-33c0568addc7",
   "metadata": {},
   "source": [
    "Similarly, we can create a tensor with all 1s by invoking ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b43fe68-8653-4c2e-83b7-903a3cb9e17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355dd9f7-3b03-462f-965b-58fc1dace50d",
   "metadata": {},
   "source": [
    "### sampling from probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e298f2-4250-4574-9613-8c15dc908bde",
   "metadata": {},
   "source": [
    "We often wish to sample each element randomly (and independently) from a given probability distribution. For ex: the parameters of neural networks are often initialized randomly. The following snippet creates a tensor with elements drawn from a **standard Gaussian (normal) distribution** with **mean 0** and **standard deviation 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c973008-1ac6-4124-827c-75f27b54b1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0353, -0.9426,  0.2231,  0.5707],\n",
       "        [ 1.4171, -0.3089,  1.2749,  2.2221],\n",
       "        [-0.6887,  0.7303,  0.1220,  0.6092]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20342c2-78bf-4988-ac99-273cd6bf9f4b",
   "metadata": {},
   "source": [
    "### create tensors from list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027efc7-b19b-4d94-9193-e9c2d57ea4f0",
   "metadata": {},
   "source": [
    "Finally, we can construct tensors by supplying the exact values for each element by supplying (possibly nested) Python list(s) containing numerical literals. Here, we construct a **matrix** with a **list of lists**, where the **outermost list** corresponds to **axis 0**, and the **inner list** corresponds to **axis 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b1412-dffc-4b0b-94b0-c96dcf2fc1e9",
   "metadata": {},
   "source": [
    "## Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fe98f-ac5a-4894-adf5-32a208c5e6d7",
   "metadata": {},
   "source": [
    "As with Python lists, we can access tensor elements by indexing (starting with 0). To access an element based on its position relative to the end of the list, we can use **negative indexing**. Finally, we can access whole ranges of indices via slicing (e.g., **`X[start:stop])`**, where the returned value includes the first index (start) but not the last (stop). Finally, when only one index (or slice) is specified for a ${k^{th}}$ - order tensor, it is applied along axis 0. Thus, in the following code, **[-1]** selects the last row and **[1:3]** selects the second and third rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "582ed0c2-a15c-457c-a01f-322cef8a651b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b38ec98-2304-4603-ba72-3246048d6777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f141f56d-bc80-4933-8e70-954ea6f54425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b77aba-8a38-4a4e-bf96-4a67e77644a7",
   "metadata": {},
   "source": [
    "Beyond reading them, we can also write elements of a matrix by specifying indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b08deb-7bc7-4271-a32e-854eccd6097a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 17\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948a1fc-d170-4c07-8817-6bc8a639a818",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5323d1-bf39-4a93-bb1a-2be40f9ecf6e",
   "metadata": {},
   "source": [
    "Now that we know how to construct tensors and how to read from and write to their elements, we can begin to manipulate them with various mathematical operations. Among the most useful of these are the **`elementwise operations`**. These apply a standard scalar operation to each element of a tensor. For functions that take two tensors as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements. We can create an elementwise function from any function that maps from **a scalar to a scalar**.\n",
    "\n",
    "In mathematical notation, we denote such **`unary scalar operators`** (taking one input) by the signature $f:\\mathbb{R}\\to\\mathbb{R}$. This just means that **the function maps from any real number onto some other real number**. Most standard operators, including **unary** ones like ${e^{x}}$ , can be applied elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a1ce75-2185-48b1-a280-5e32ca748630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,\n",
       "        2.4155e+07, 1.0966e+03, 2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c69b12-8262-4952-b523-f7314c7c9310",
   "metadata": {},
   "source": [
    "<img src=\"binary elementwise operations.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d24a78-15d5-4768-bb3a-ebde20b74f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4.,  6., 10.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c480a0c8-2d9b-43d5-b220-ad0a64d2ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0.,  2.,  6.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc24b2c-33a5-4a27-90d9-eca3aeb8a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  8., 16.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f4b7c00-5ded-4689-b0b0-9f959ab74e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 1.0000, 2.0000, 4.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc8df8b3-4a67-438c-9c14-f08a3c92320d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4., 16., 64.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e652f5-2623-4764-9c40-562059a17b78",
   "metadata": {},
   "source": [
    "In addition to elementwise computations, we can also perform **linear algebraic operations**, such as dot products and matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805e055-5205-4d45-a7cc-d86728d3b40b",
   "metadata": {},
   "source": [
    "## Tensor Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eddbdd-5022-40c9-83a9-39698efec790",
   "metadata": {},
   "source": [
    "We can also concatenate multiple tensors, stacking them end-to-end to form a larger one. We just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate two matrices along **rows (axis 0)** instead of **columns (axis 1)**. We can see that the first output’s **axis-0 length (6)** is the sum of the two input tensors’ **axis-0 lengths (3+3)** while the second output’s **axis-1 length (8)** is the sum of the two input tensors’ **axis-1 lengths (4+4)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a87bc54-270d-4a25-9560-1b30ea43a442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d4ee5c4-1e23-4270-889f-9137bf285d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X, Y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebb0281f-e1bf-46e9-b54a-e255463aab96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d9087-247e-4044-be84-837a6d3bef93",
   "metadata": {},
   "source": [
    "## Binary Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888e6cd-e0ca-4b36-882c-7d7474569cf2",
   "metadata": {},
   "source": [
    "Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example. For each position i, j, if X[i, j] and Y[i, j] are equal, then the corresponding entry in the result takes value 1, otherwise it takes value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44f8b11f-8837-4681-9357-edfa4a31e363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X==Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296da465-fece-41e8-a1fc-99916eef35a9",
   "metadata": {},
   "source": [
    "## Sum all the elements in a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ef753-1a5f-4683-908a-61b613c9f1ae",
   "metadata": {},
   "source": [
    "Summing all the elements in the tensor yields a tensor with only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2fda70b-6b9e-4988-b393-0389a787cc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a84f78a-8d06-406a-9bb9-86daa735b2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd36388-dc2c-4c39-9da5-80602d676e32",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920f7b6-6a81-4d92-bef3-d32a6042bf88",
   "metadata": {},
   "source": [
    "By now, you know how to perform **elementwise binary operations** on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise binary operations by invoking the broadcasting mechanism. Broadcasting works according to the following two-step procedure: \n",
    "* (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape.\n",
    "* (ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90ec69e7-a64d-43f7-b3cc-750cff0748db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f159209-f905-4b3b-a8de-2b6a90bb87ef",
   "metadata": {},
   "source": [
    "Since a and b are 3 X 1 and 1 X 2 matrices, respectively, their shapes do not match up. Broadcasting produces a larger 3 x 2 matrix by replicating matrix a along the columns and matrix b along the rows before adding them elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df3912da-564f-4274-8efb-de3a8db630c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d79b9-9921-42d7-8deb-2bb8d7346a1a",
   "metadata": {},
   "source": [
    "## Saving Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0350ccf-edb6-48d8-83a1-98d6a67efb47",
   "metadata": {},
   "source": [
    "Running operations can cause new memory to be allocated to host results. For example, if we write **Y = X + Y**, we dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. We can demonstrate this issue with Python’s **`id()`** function, which gives us the exact address of the referenced object in memory. Note that after we run **Y = Y + X**, **`id(Y)`** points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then points Y to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "665d8c22-ddcb-4cfa-9d83-7aa191197fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4563788144\n",
      "4570587264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "print(before)\n",
    "\n",
    "Y = Y + X\n",
    "\n",
    "after = id(Y)\n",
    "print(after)\n",
    "\n",
    "before == after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf767388-e69c-4ee6-ae41-8fb8a8277ee3",
   "metadata": {},
   "source": [
    "This might be **undesirable** for two reasons: \n",
    "* First, we do not want to run around **allocating memory unnecessarily** all the time. In machine learning, we often have hundreds of megabytes of parameters and update all of them multiple times per second. Whenever possible, we want to **perform these updates `in place`**.\n",
    "* Second, we might point at the same parameters from multiple variables. If we do not update in place, we must be careful to update all of these references, lest we spring a **memory leak** or inadvertently refer to **stale parameters**.\n",
    "\n",
    "1. **Memory Leak:** A memory leak occurs when a program allocates memory but fails to release it when it's no longer needed. In the context of machine learning parameters:\n",
    "   - If we create new copies of parameters instead of updating in place, we might forget to delete the old versions.\n",
    "   - Over time, this can lead to accumulation of unused memory, causing the program to consume more and more resources.\n",
    "\n",
    "3. **Stale Parameters:** \"Stale\" refers to outdated or obsolete data. In this context:\n",
    "   - If multiple variables point to the same parameters, and we update by creating a new copy instead of modifying the original, some variables might still reference the old (stale) version.\n",
    "   - This can lead to inconsistencies in the model, where different parts of the program are using different versions of the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b3075c0-f690-4f1a-afaa-1fa526bcc18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "id(Z): 4565669184\n",
      "tensor([[ 2.,  3.,  8.,  9.],\n",
      "        [ 9., 12., 15., 18.],\n",
      "        [20., 21., 22., 23.]])\n",
      "id(Z): 4565669184\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print(Z)\n",
    "print('id(Z):', id(Z))\n",
    "\n",
    "Z[:] = X + Y\n",
    "print(Z)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f120044-5947-4960-a921-589d61ab605d",
   "metadata": {},
   "source": [
    "If the value of **X** is not reused in subsequent computations, we can also use **`X[:] = X + Y`** or **`X += Y`** to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eafa387-2abf-44b9-8202-8c4aafe34aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "after = id(X)\n",
    "after == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f1dad-84cf-4ec8-a30f-5d396aa8bdcf",
   "metadata": {},
   "source": [
    "## Conversion to Other Python Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872442a-2802-4106-9998-299856843a5e",
   "metadata": {},
   "source": [
    "Converting to a **NumPy tensor (ndarray)**, or vice versa, is easy. The torch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e57b648e-c693-4882-b6ef-278d52c5ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  3.,  8.,  9.],\n",
      "        [ 9., 12., 15., 18.],\n",
      "        [20., 21., 22., 23.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X)\n",
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bc25f-4ee5-438a-b377-47023088a72d",
   "metadata": {},
   "source": [
    "To convert a **size-1 tensor** to a Python scalar, we can invoke the **`item`** function or Python’s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d54c9d0a-0f37-4b9e-8cf9-583d161c6bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985326f3-2bdc-41f7-8f29-c31fd67863a8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2152b-2c0c-4429-b4ba-75566c2f9c8a",
   "metadata": {},
   "source": [
    "The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5406ec14-3d96-46eb-a925-bb0030a8f82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8abe2017-a028-41d4-84d0-4addbaec9607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X < Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f3a91cc-64d0-42df-9234-a3d82150f59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X > Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bab28a-cc04-48c2-981d-7d0ca7cb4fee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e044a6-e520-48d1-b806-ff02c6e222c1",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f1186-8dd3-48f0-8c9b-98d93480d704",
   "metadata": {},
   "source": [
    "So far, we have been working with synthetic data that arrived in ready-made tensors. However, to apply deep learning in the wild we must extract messy data stored in arbitrary formats, and preprocess it to suit our needs. Fortunately, the pandas library can do much of the heavy lifting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "612cf52a-54e5-4dd2-9b6e-23c8f3f3d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''\n",
    "        NumRooms,RoofType,Price\n",
    "        NA,NA,127500\n",
    "        2,NA,106000\n",
    "        4,Slate,178100\n",
    "        NA,NA,140000\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "486dd318-0ae0-4340-954b-b2843b32c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          NumRooms RoofType   Price\n",
      "0               NA      NaN  127500\n",
      "1                2      NaN  106000\n",
      "2                4    Slate  178100\n",
      "3               NA      NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2287efb-8233-4e6c-a597-9aa8a77401bd",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ea08a-bbe7-4877-b68a-9d40cdc179f0",
   "metadata": {},
   "source": [
    "In **supervised learning**, we train models to predict a designated target value, given some set of input values. Our first step in processing the dataset is to separate out columns corresponding to input versus target values. We can select columns either by name or via integer-location based indexing (**iloc**).\n",
    "\n",
    "You might have noticed that pandas replaced all CSV entries with value NA with a special NaN (not a number) value. This can also happen whenever an entry is empty, e.g., “3,,,270000”. These are called **missing values** and they are the “**bed bugs**” of data science, a persistent menace that you will confront throughout your career. Depending upon the context, missing values might be handled either via **imputation** or **deletion**. Imputation replaces missing values with estimates of their values while deletion simply discards either those rows or those columns that contain missing values.\n",
    "\n",
    "Here are some common imputation heuristics. For categorical input fields, we can treat NaN as a category. Since the RoofType column takes values Slate and NaN, pandas can convert this column into two columns RoofType_Slate and RoofType_nan. A row whose roof type is Slate will set values of RoofType_Slate and RoofType_nan to 1 and 0, respectively. The converse holds for a row with a missing RoofType value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32deb96e-aa9c-40a2-ad38-7d407f7b264a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms_        2</th>\n",
       "      <th>NumRooms_        4</th>\n",
       "      <th>NumRooms_        NA</th>\n",
       "      <th>NumRooms_nan</th>\n",
       "      <th>RoofType_Slate</th>\n",
       "      <th>RoofType_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           NumRooms_        2          NumRooms_        4  \\\n",
       "0                       False                       False   \n",
       "1                        True                       False   \n",
       "2                       False                        True   \n",
       "3                       False                       False   \n",
       "\n",
       "           NumRooms_        NA          NumRooms_nan  RoofType_Slate  \\\n",
       "0                         True                 False           False   \n",
       "1                        False                 False           False   \n",
       "2                        False                 False            True   \n",
       "3                         True                 False           False   \n",
       "\n",
       "   RoofType_nan  \n",
       "0          True  \n",
       "1          True  \n",
       "2         False  \n",
       "3          True  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada473c8-23d7-4881-aac8-3bf09265fd00",
   "metadata": {},
   "source": [
    "For missing numerical values, one common heuristic is to replace the NaN entries with the **mean** value of the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5045f9d0-1330-40fc-b68e-05422bb8ad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms_        2</th>\n",
       "      <th>NumRooms_        4</th>\n",
       "      <th>NumRooms_        NA</th>\n",
       "      <th>NumRooms_nan</th>\n",
       "      <th>RoofType_Slate</th>\n",
       "      <th>RoofType_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           NumRooms_        2          NumRooms_        4  \\\n",
       "0                       False                       False   \n",
       "1                        True                       False   \n",
       "2                       False                        True   \n",
       "3                       False                       False   \n",
       "\n",
       "           NumRooms_        NA          NumRooms_nan  RoofType_Slate  \\\n",
       "0                         True                 False           False   \n",
       "1                        False                 False           False   \n",
       "2                        False                 False            True   \n",
       "3                         True                 False           False   \n",
       "\n",
       "   RoofType_nan  \n",
       "0          True  \n",
       "1          True  \n",
       "2         False  \n",
       "3          True  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.fillna(inputs.mean())\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf79ac4-39e1-4e88-9810-3198260c46aa",
   "metadata": {},
   "source": [
    "## Conversion to the Tensor Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057927d8-695f-4aaa-bbf3-570c3a671d30",
   "metadata": {},
   "source": [
    "Now that all the entries in **inputs** and **targets** are numerical, we can load them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9187549-8d8e-4594-9518-09515f9ad8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
    "y = torch.tensor(targets.to_numpy(dtype=float))\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc4774-d91b-4d94-8fe0-6228e87a7608",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03966950-f790-4868-8ccb-7532cf3df32e",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81e88d-101a-4e3c-8e9f-ea1670d470ad",
   "metadata": {},
   "source": [
    "By now, we can load datasets into tensors and manipulate these tensors with basic mathematical operations. To start building sophisticated models, we will also need a few tools from linear algebra. This section offers a gentle introduction to the most essential concepts, starting from scalar arithmetic and ramping up to matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6413c38-a37b-48e9-9975-cc253a67d31a",
   "metadata": {},
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bde56-df27-42e7-a7e0-e411414f645d",
   "metadata": {},
   "source": [
    "<img src=\"scalars.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17a716fb-df74-4d77-a6cf-8428b9ec719e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbe6ff-fd68-4758-b8f3-2dd4bbbab979",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d3e2b-93fa-4d48-9567-2c62bfb86a62",
   "metadata": {},
   "source": [
    "<img src=\"vectors1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "435ad570-3048-41ee-acd5-9be890a721c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce81ac-60d4-40c3-a102-831ceb64b33f",
   "metadata": {},
   "source": [
    "<img src=\"vectors2.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "917a203c-e4e1-4e87-b777-4cfe7affa63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b4963-ee8a-4e7c-865f-43e970a296bf",
   "metadata": {},
   "source": [
    "<img src=\"vectors3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9bb5b224-233d-4cdb-a04d-be98d37392e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b53d0-4791-427c-a9c0-36e354a3e103",
   "metadata": {},
   "source": [
    "We can also access the length via the **`shape`** attribute. The shape is a tuple that indicates a tensor’s length along each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54d5d643-a0eb-465e-9118-26b464d15577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2db79-b5d7-4251-b82a-54800d143495",
   "metadata": {},
   "source": [
    "Oftentimes, the word **“dimension”** gets overloaded to mean both **the number of axes** and **the length along a particular axis**. To avoid this confusion, we use **order** to refer to the number of axes and dimensionality exclusively to refer to the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e050757-000b-437a-b62c-96a8af875a34",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab14c18-59dc-4047-b0d0-8e7013012e26",
   "metadata": {},
   "source": [
    "<img src=\"matrices.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4e50fe1-894f-417f-983c-f278c07cb303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbfcfe-4116-42b4-be8b-02923d3497bc",
   "metadata": {},
   "source": [
    "<img src=\"matrices2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe2411c0-d20d-4a49-a3af-31f260bba28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a408076-a9c7-45df-8eb5-4a72c0df5155",
   "metadata": {},
   "source": [
    "**Symmetric matrices** is when ${A = A^{T}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60e29bb0-1739-4d66-a784-ffbd3ce5a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5d1ab-1b15-48b7-a113-b804a9d84906",
   "metadata": {},
   "source": [
    "Matrices are useful for representing datasets. Typically, rows correspond to individual records and columns correspond to distinct attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c225b-9b2e-41f3-b71c-bd1f373a40da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
